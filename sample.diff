diff --git a/src/gpt_review/_ask.py b/src/gpt_review/_ask.py
index cc96c29..29c9e84 100644
--- a/src/gpt_review/_ask.py
+++ b/src/gpt_review/_ask.py
@@ -3,7 +3,6 @@
 import os
 import time
 from typing import Dict, List, Optional
-from typing_extensions import override
 from knack import CLICommandsLoader
 from knack.arguments import ArgumentsContext
 from knack.commands import CommandGroup
@@ -13,105 +12,15 @@
 from azure.identity import DefaultAzureCredential
 from azure.keyvault.secrets import SecretClient
 from openai.error import RateLimitError
-from langchain.llms import AzureOpenAI
-from langchain.embeddings import OpenAIEmbeddings
-from llama_index import (
-    GPTVectorStoreIndex,
-    LangchainEmbedding,
-    ServiceContext,
-    LLMPredictor,
-    SimpleDirectoryReader,
-)
-from llama_index.indices.base import BaseGPTIndex
 
 from gpt_review._command import GPTCommandGroup
+from gpt_review._llama_index import _ask_doc
 import gpt_review.constants as C
 
 
 DEFAULT_KEY_VAULT = "https://dciborow-openai.vault.azure.net/"
 
 
-def _ask_doc(question: str, files: List[str]) -> str:
-    """
-    Ask GPT a question.
-
-    Args:
-        question (List[str]): The question to ask.
-        files (List[str]): The files to search.
-
-    Returns:
-        Dict[str, str]: The response.
-    """
-    documents = SimpleDirectoryReader(input_files=files).load_data()
-    index = _document_indexer(documents)
-
-    return index.as_query_engine().query(question).response  # type: ignore
-
-
-def _document_indexer(documents) -> BaseGPTIndex:
-    """
-    Create a document indexer.
-
-    Deployment names include: "gpt-35-turbo", "text-davinci-003"
-
-    Args:
-        documents (List[Document]): The documents to index.
-        azure (bool): Whether to use Azure OpenAI.
-
-    Returns:
-        GPTVectorStoreIndex: The document indexer.
-    """
-    _load_azure_openai_context()
-
-    llm = AzureGPT35Turbo(  # type: ignore
-        deployment_name="gpt-35-turbo",
-        model_kwargs={
-            "api_key": openai.api_key,
-            "api_base": openai.api_base,
-            "api_type": "azure",
-            "api_version": "2023-03-15-preview",
-        },
-        max_retries=10,
-    )
-    llm_predictor = LLMPredictor(llm=llm)
-
-    embedding_llm = LangchainEmbedding(
-        OpenAIEmbeddings(
-            model="text-embedding-ada-002",
-        ),  # type: ignore
-        embed_batch_size=1,
-    )
-
-    service_context = ServiceContext.from_defaults(
-        llm_predictor=llm_predictor,
-        embed_model=embedding_llm,
-    )
-    return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)
-
-
-class AzureGPT35Turbo(AzureOpenAI):
-    """Azure OpenAI Chat API."""
-
-    @property
-    @override
-    def _default_params(self):
-        """
-        Get the default parameters for calling OpenAI API.
-        gpt-35-turbo does not support best_of, logprobs, or echo.
-        """
-        normal_params = {
-            "temperature": self.temperature,
-            "max_tokens": self.max_tokens,
-            "top_p": self.top_p,
-            "frequency_penalty": self.frequency_penalty,
-            "presence_penalty": self.presence_penalty,
-            "n": self.n,
-            "request_timeout": self.request_timeout,
-            "logit_bias": self.logit_bias,
-        }
-        return {**normal_params, **self.model_kwargs}
-
-
 def validate_parameter_range(namespace) -> None:
     """
     Validate the following parameters:
@@ -161,13 +70,15 @@ def _ask(
     presence_penalty: float = C.PRESENCE_PENALTY_DEFAULT,
     files: Optional[List[str]] = None,
     fast: bool = False,
+    large: bool = False,
 ) -> Dict[str, str]:
     """Ask GPT a question."""
+    _load_azure_openai_context()
 
     prompt = " ".join(question)
 
     if files:
-        response = _ask_doc(prompt, files)
+        response = _ask_doc(prompt, files, fast=fast, large=large)
     else:
         response = _call_gpt(
             prompt=prompt,
@@ -177,6 +88,7 @@ def _ask(
             frequency_penalty=frequency_penalty,
             presence_penalty=presence_penalty,
             fast=fast,
+            large=large,
         )
     return {"response": response}
 
@@ -191,8 +103,8 @@ def _load_azure_openai_context() -> None:
     - Without setting the environment variables, the integration tests fail.
     - Without setting the openai package variables, the cli tests fail.
     """
-    openai.api_type = "azure"
-    openai.api_version = "2023-03-15-preview"
+    openai.api_type = os.environ["OPENAI_API_TYPE"] = "azure"
+    openai.api_version = os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
 
     if os.getenv("AZURE_OPENAI_API"):
         openai.api_base = os.environ["OPENAI_API_BASE"] = os.getenv("AZURE_OPENAI_API")  # type: ignore
@@ -215,6 +127,7 @@ def _call_gpt(
     retry=0,
     messages=None,
     fast: bool = False,
+    large: bool = False,
 ) -> str:
     """
     Call GPT-4 with the given prompt.
@@ -229,15 +142,14 @@ def _call_gpt(
         retry (int, optional): The number of times to retry the request. Defaults to 0.
         messages (List[Dict[str, str]], optional): The messages to send to GPT-4. Defaults to None.
         fast (bool, optional): Whether to use the fast model. Defaults to False.
+        large (bool, optional): Whether to use the large model. Defaults to False.
 
     Returns:
         str: The response from GPT-4.
     """
-    _load_azure_openai_context()
-
     messages = messages or [{"role": "user", "content": prompt}]
     try:
-        engine = _get_engine(prompt, fast)
+        engine = _get_engine(prompt, max_tokens=max_tokens, fast=fast, large=large)
         logging.info("Model Selected based on prompt size: %s", engine)
 
         logging.info("Prompt sent to GPT: %s\n", prompt)
@@ -259,20 +171,43 @@ def _call_gpt(
         raise RateLimitError("Retry limit exceeded") from error
 
 
-def _get_engine(prompt: str, fast: bool = False) -> str:
+def _get_engine(prompt: str, max_tokens: int, fast: bool = False, large: bool = False) -> str:
     """
     Get the Engine based on the prompt length.
     - when greater then 8k use gpt-4-32k
     - otherwise use gpt-4
     - enable fast to use gpt-35-turbo for small prompts
+
+    Args:
+        prompt (str): The prompt to send to GPT-4.
+        max_tokens (int): The maximum number of tokens to generate.
+        fast (bool, optional): Whether to use the fast model. Defaults to False.
+        large (bool, optional): Whether to use the large model. Defaults to False.
+
+    Returns:
+        str: The engine to use.
     """
-    if len(prompt) > 8000:
+    tokens = _count_tokens(prompt)
+    if large or tokens + max_tokens > 8000:
         return "gpt-4-32k"
-    if len(prompt) > 4000:
+    if tokens + max_tokens > 4000:
         return "gpt-4"
     return "gpt-35-turbo" if fast else "gpt-4"
 
 
+def _count_tokens(prompt) -> int:
+    """
+    Determine number of tokens in prompt.
+
+    Args:
+        prompt (str): The prompt to send to GPT-4.
+
+    Returns:
+        int: The number of tokens in the prompt.
+    """
+    return int(len(prompt) / 4 * 3)
+
+
 class AskCommandGroup(GPTCommandGroup):
     """Ask Command Group."""
 
@@ -291,6 +226,12 @@ def load_arguments(loader: CLICommandsLoader) -> None:
                 default=False,
                 action="store_true",
             )
+            args.argument(
+                "large",
+                help="Use gpt-4-32k for prompts.",
+                default=False,
+                action="store_true",
+            )
             args.argument(
                 "temperature",
                 type=float,
diff --git a/src/gpt_review/_llama_index.py b/src/gpt_review/_llama_index.py
new file mode 100644
index 0000000..6137a26
--- /dev/null
+++ b/src/gpt_review/_llama_index.py
@@ -0,0 +1,96 @@
+"""Wrapper for Llama Index."""
+from typing import List
+from typing_extensions import override
+import openai
+
+from langchain.chat_models import AzureChatOpenAI
+from langchain.embeddings import OpenAIEmbeddings
+from langchain.llms import AzureOpenAI
+from llama_index import GPTVectorStoreIndex, LLMPredictor, LangchainEmbedding, ServiceContext, SimpleDirectoryReader
+from llama_index.indices.base import BaseGPTIndex
+
+
+def _ask_doc(question: str, files: List[str], fast: bool = False, large: bool = False) -> str:
+    """
+    Ask GPT a question.
+    Args:
+        question (List[str]): The question to ask.
+        files (List[str]): The files to search.
+        fast (bool, optional): Whether to use the fast model. Defaults to False.
+        large (bool, optional): Whether to use the large model. Defaults to False.
+
+    Returns:
+        Dict[str, str]: The response.
+    """
+    documents = SimpleDirectoryReader(input_files=files).load_data()
+    index = _document_indexer(documents, fast=fast, large=large)
+
+    return index.as_query_engine().query(question).response  # type: ignore
+
+
+def _document_indexer(
+    documents,
+    fast: bool = False,
+    large: bool = False,
+) -> BaseGPTIndex:
+    """
+    Create a document indexer.
+    Deployment names include: "gpt-4", "gpt-4-32", "gpt-35-turbo", "text-davinci-003"
+    Args:
+        documents (List[Document]): The documents to index.
+        fast (bool, optional): Whether to use the fast model. Defaults to False.
+        large (bool, optional): Whether to use the large model. Defaults to False.
+
+    Returns:
+        GPTVectorStoreIndex: The document indexer.
+    """
+    llm_type = AzureGPT35Turbo if fast else AzureChatOpenAI
+    llm_name = "gpt-35-turbo" if fast else "gpt-4-32k" if large else "gpt-4"
+    llm = llm_type(  # type: ignore
+        deployment_name=llm_name,
+        model_kwargs={
+            "api_key": openai.api_key,
+            "api_base": openai.api_base,
+            "api_type": "azure",
+            "api_version": "2023-03-15-preview",
+        },
+        max_retries=10,
+    )
+
+    llm_predictor = LLMPredictor(llm=llm)
+
+    embedding_llm = LangchainEmbedding(
+        OpenAIEmbeddings(
+            model="text-embedding-ada-002",
+        ),  # type: ignore
+        embed_batch_size=1,
+    )
+
+    service_context = ServiceContext.from_defaults(
+        llm_predictor=llm_predictor,
+        embed_model=embedding_llm,
+    )
+    return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)
+
+
+class AzureGPT35Turbo(AzureOpenAI):
+    """Azure OpenAI Chat API."""
+
+    @property
+    @override
+    def _default_params(self):
+        """
+        Get the default parameters for calling OpenAI API.
+        gpt-35-turbo does not support best_of, logprobs, or echo.
+        """
+        normal_params = {
+            "temperature": self.temperature,
+            "max_tokens": self.max_tokens,
+            "top_p": self.top_p,
+            "frequency_penalty": self.frequency_penalty,
+            "presence_penalty": self.presence_penalty,
+            "n": self.n,
+            "request_timeout": self.request_timeout,
+            "logit_bias": self.logit_bias,
+        }
+        return {**normal_params, **self.model_kwargs}
diff --git a/tests/test_llama_index.py b/tests/test_llama_index.py
new file mode 100644
index 0000000..6816464
--- /dev/null
+++ b/tests/test_llama_index.py
@@ -0,0 +1,23 @@
+"""Tests for the Llame Index Package."""
+import pytest
+
+from gpt_review._ask import _load_azure_openai_context
+from gpt_review._llama_index import _ask_doc
+
+
+def ask_doc_test() -> None:
+    _load_azure_openai_context()
+    question = "What is the name of the package?"
+    files = ["src/gpt_review/__init__.py"]
+    _ask_doc(question, files, fast=True)
+
+
+def test_ask_doc(mock_openai) -> None:
+    """Unit Test for the ask_doc function."""
+    ask_doc_test()
+
+
+@pytest.mark.integration
+def test_int_ask_doc() -> None:
+    """Integration Test for the ask_doc function."""
+    ask_doc_test()
